# Multimodal LLM Project

## Overview
This project aims to develop an innovative multi-modal large language model (LLM) system that integrates video, image, and text processing capabilities. Our goal is to create a flexible and comprehensive AI solution applicable across various domains.

## Features
- **Video Input to Caption Output**: Utilizes advanced models like Vid2Seq to process video inputs and generate descriptive captions.
- **Image Input to Caption Output**: Leverages pre-trained models to interpret and describe image content.
- **Text Input to Text Output**: Employs state-of-the-art language models for coherent and contextually relevant text generation.

## Current Focus
The current implementation focuses on the video captioning component, with plans to integrate image and text processing in future iterations.

## Technologies Used
- Python
- PyTorch
- Hugging Face Transformers
- OpenCV
- Gradio

## Setup and Installation
1. Clone the repository:
git clone https://github.com/Aryand43/multimodal-llm-project.git
