# Multimodal LLM Project

## Overview
This project aims to develop an innovative multi-modal large language model (LLM) system that integrates video, image, and text processing capabilities. The goal is to create a flexible and comprehensive AI solution applicable across various domains.

## Features
- **Video Input to Caption Output**: Utilizes advanced models like Vid2Seq to process video inputs and generate descriptive captions.
- **Image Input to Caption Output**: Leverages pre-trained models to interpret and describe image content.
- **Text Input to Text Output**: Employs state-of-the-art language models for coherent and contextually relevant text generation.

## Current Focus
The current implementation focuses on the video captioning component, with plans to integrate image and text processing in future iterations.

## Technologies Used
- Python
- PyTorch
- Hugging Face Transformers
- OpenCV
- Gradio

## Setup and Installation
1. Clone the repository:
git clone https://github.com/Aryand43/V2C-llm-project.git

2. Install required packages:
pip install -r requirements.txt

3. Open and run the Jupyter notebook `Video_Captioning_System.ipynb` in Google Colab or your local environment.

## Usage
1. Upload a video file through the Gradio interface.
2. The system will process the video and generate a caption describing its content.
3. View the generated caption in the output text box.

## Future Development
- Integration of image captioning capabilities
- Implementation of text-to-text generation features
- Development of a unified interface for all modalities

## Contributing
Contributions to this project are welcome. Please feel free to submit issues and pull requests.

## Contact
Aryan D - aryan.dutt43@gmail.com

Project Link: https://github.com/Aryand43/multimodal-llm-project

## License
This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.
